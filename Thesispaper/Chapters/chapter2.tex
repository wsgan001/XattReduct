\chapter{聚类与属性选择概述}
\label{chapter:relationresearch}
本章将主要介绍聚类和属性选择的基本概念，并将介绍聚类和属性选择的研究现状及大体分类。
\section{聚类算法}
%论文：半监督聚类集成模型研究
\subsection{聚类概念与相关定义}

聚类分析是一种基本的无监督学习方法，同时也是多元统计分析的一个重要研究内容，已被广泛的运用到许多应用领域中，例如：模式识别，数据挖掘，图像音频处理，信息论，生物信息学等领域\cite{Mirkin2005,Plasse2007,Chopra2008,Tseng2005,Li2008,Li2008a}。
所谓聚类分析就是将无标签数据按照某种度量，根据数据自身的特征，组织成具有不同特点的集簇\cite{Alpaydin2009}。 聚类分析的基本目标是发现样本集合的自然分组方法\cite{Alpaydin2009}。为此，必须先定义度量尺度，借以度量对象之间的联系，这个定量尺度就叫做对象相似性。为了从复杂的数据集中产生出比较简单的结构，多数做法都要求有一个“接近程度”或“相似性”的量度。在选择相似性量度时，通常带有相当大的主观性。
%需要考虑的主要问题包括变量的性质（离散型，连续型，二值型），测量值的尺度（顺序尺度，间隔尺度），以及关于研究领域的先验知识。

%由于聚类分析能够快速找出样本数据中蕴含的结构信息，因此

%\subsection{相似性度量与表示}
\subsection{聚类算法的分类}
%半监督聚类集成模型研究
聚类方法主要包括基于划分的方法、层次方法、基于密度的方法、基于网格的方法以及基于模型的方法等。
%，其代表算法如图~\ref{fig:chapter2:cluster} 所示。
%
%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \center
%  \includegraphics[width=5.5in]{./Chapters/cluster.eps}\\
%  \caption{无监督聚类方法的分类示意图}\label{fig:chapter2:cluster}
%\end{figure}

\begin{enumerate}[1)]
  \item 基于划分方法：通过给定待对象为$n$的数据集以及期望生成集簇的数目$k(k\leqslant n)$，根据划分策略将样本分成$k$个集合，算法过程通过重复迭代将对象分配到最近的类中，直到聚类结果达到相应的收敛准则。典型的算法有：k-Means算法、CLARANS算法以及PAM算法。
  \item 基于层次方法：采用分裂或凝聚的方式在不同层次对数据进行划分，聚类结果以树形聚类图表示。凝聚聚类初始将每个数据作为一个单独的类，随后逐层合并对象直到所有对象被合并为一个类。分裂聚类则相反，层次聚类算法的典型算法有Birch算法和Chameleno算法等。
  \item 基于密度方法：主要依据数据集的密度分布来完成聚类过程。密度聚类的优点是不受数据集的形状影响，并可以过滤噪声对象。算法将密度大的区域的对象划分到一起，密度小的对象则被分离开来，DBSCAN算法和OPTICS算法是基于密度的聚类代表性算法。
  \item 基于网格方法：将数据首先转化为网格结构，然后基于网格结构进行聚类，该算法拥有处理高效以及算法性能与数据对象的数量无关等优点。典型的算法有：Sting算法、Clique算法以及WaveCluster算法。
  \item 基于模型方法：基于数学模型，在给定的样本和模型间建立关联的一种方法。EM算法、COBWEB算法、SOM算法和ART神经网络算法是基于模型聚类方法的代表性算法。
\end{enumerate}


%
%基于划分方法通过给定待聚类大小为$n$的数据集以及期望生成簇的数目$k(k\leqslant n)$，根据划分策略将样本分成$k$个集合，算法过程通过重复迭代将对象分配到最近的类中，直到聚类结果达到相应的收敛准则。K-means 算法、CLARANS算法以及PAM算法是划分方法的代表性算法。
%
%基于层次的聚类算法采用分裂或凝聚的方式在不同层次对数据进行划分，聚类结果以树形聚类图表示。凝聚聚类初始将每个数据作为一个单独的类，随后逐层合并对象直到所有对象被合并为一个类。分裂聚类则相反，层次聚类算法的代表算法有Birch 算法和Chameleno算法等。
%
%基于密度的聚类算法主要依据数据的密度分布来完成聚类过程。密度聚类的优点是不受数据集的形状影响，并可以过滤噪声点。算法将密度大的区域的对象划分到一起，密度小的对象则被分离开来，DBSCAN算法和OPTICS算法是基于密度的聚类典型算法。
%
%基于网格的聚类算法将数据首先转化为网格结构，然后基于网格结构进行聚类，该算法拥有处理高效以及算法性能与数据对象的数量无关等优点。Sting 算法、Clique算法以及WaveCluster算法是该类算法的代表。
%
%基于模型的聚类算法是基于一种数学模型，在给定的样本和模型间建立关联的一种方法。EM算法、COBWEB、 SOM和ART神经网络算法是基于模型聚类方法的典型算法。
%
%




%\subsection{聚类性能评价标准}
%
%常见的聚类结果评价方法有内部评价法、外部评价法和相对评价法\cite{Han2006}。外部评价法是基于数据集分布已知的情况，常用F-measure值来进行评价。
%
%
%
%F-measure是利用查准率和查全率的概率来对聚类结果的质量进行评价的方法。它同时考虑了准确率和召回率两种评价标准来对聚类的质量进行评价的，准确率用以评价算法的准确性，召回率可评价算法的完备性。准确率指对于一个实验结果的类别来说，正确归类的对象数和实验结果此类别的对象数的比值；召回率指对于某个已知类别来说，正确归类的对象数和这个已知类别中的对象个数的比值\cite{Shi2010}。
%
%设某个已知类别为$C_i$，聚类后的某个类别为$C_j$，则类别$C_i$和类别$C_j$的F-measure值为公式为
%\begin{equation}\label{equ:chapter2:fmeasure:CiCj}
%  F(C_i,C_j)=\frac{2\times P\times R}{P+R}
%\end{equation}
%用来评价聚类质量的F-measure值就是计算求平均值
%\begin{equation}\label{equ:chapter2:fmeasure}
%  F=\sum_{i}\frac{N_i}{N}\max_{r}\{F(C_i,C_r)\}
%\end{equation}
%
%其中
%\begin{equation}\label{equ:chapter2:precision}
%  P=precision(C_i,C_j)=\frac{N_{ij}}{N_i}
%\end{equation}
%\begin{equation}\label{equ:chapter2:recall}
%  R=recall(C_i,C_j)=\frac{N_{ij}}{N_j}
%\end{equation}
%
%$N_i$是数据集中类标为$i$的所有样本数量，$N_j$ 为聚类结果中类标为$j$的所有对象数量，$N_ij$为聚类结果类标为$j$ 的簇与数据集中类标为$i$的类得交集所含样本的数量。
%
%F-measure值同时考虑了准确率和召回率这两个评价标准，是评价更有效。F-measure值是在0和1 之间的数值，F-measure值越高证明聚类的质量越好，相反，F-measure值越低证明聚类的质量越差。
%
%
%互信息是信息论中的度量标准，是指两件事物之间的关联度信息。在这里用归一化互信息来衡量聚类的质量时，是指已知的数据集的类别信息与试验后的聚类结果进行比较后的关联度信息，互信息的值越大表明关联度越强，表示实验的聚类质量比较优良，反之，互信息值越小表示关联度越弱，表示实验的聚类质量比较差，因此互信息可以有效的评价聚类质量
%
%假设$\pi^a$和$\pi^b$是分别是两个聚类结果，$k^a$ 和$k^b$ 分别表示$\pi^a$和$\pi^b$的簇的数量，$n_i$ 表示$\pi^a$中归属于类$C^a_i$的对象数量，$n_j$表示$\pi^b$中归属于类$C^b_j$的对象数量，$n_ij$表示同时属于$\pi^a$中类$C^a_i$ 和$\pi^b$中类$C^b_j$ 的对象数量，$n$表示总共的对象个数。则范围是$[0,1]$ 的归一化互信息定义为
%\begin{equation}\label{equ:chapter2:nmi}
%  \mathbf{\Phi}^{NMI}(\pi^a,\pi^b)=\frac{\sum^{k^a}_{i=1}\sum^{k^b}_{j=1}\log\bigg(\frac{n\cdot n_{ij}}{n_i n_j}\bigg)}{\sqrt{\bigg(\sum^{k^a}_{i=1}n_i\log\frac{n_i}{n}\bigg)\bigg(\sum^{k^b}_{j=1}n_j\log\frac{n_j}{n}\bigg)}}
%\end{equation}
%
%归一化后的互信息取值在0和1之间，这个值越高表示聚类效果越好，考虑极值情况，互信息等于1表明实验结果与已知类别完全吻合，互信息等于0表明实验的结果随机分布。
%
%
%
%
%内部评价方法是利用数据集自有特征和测量值对聚类结果进行评价，文献\cite{YangYan2008}给出了一种改进的Hubert's $\Gamma$ 统计法：
%
%\begin{equation}\label{equ:chapter2:gamma}
%  \Gamma = \frac{1}{M}\sum_{i=1}^{N-1}\sum_{j=i+1}^{N}P(i,j)Q(i,j)
%\end{equation}
%
%\begin{equation}\label{equ:chapter2:gamma:q}
%  Q(i,j)=\left\{
%   \begin{array}{cl}
%   1,&\text{如果$x_i$和$x_j$属于不同的簇，}i,j=1,2,\dots,N\\
%   0,&\text{其它}\\
%   \end{array}
%   \right.
%\end{equation}
%
%其中，$P$是一个$N\times N$的矩阵，$M=N\times (N-1)/2$，$P$中的元素代表数据对象的差异度，$Q$是一个$N\times N$的矩阵，元素为0 和1，如果$x_i$ 和$x_j$ 属于不同的类则为1，否则为0。$\Gamma$反映了数据集的分布与聚类划分是否相适应。
%
%相对评价法主要有Dumn指数、DB指数、SD有效性指数、RMSSDT/SPR/RS/CD指数等，其中Ocq(Overall Cluster Quality)，即聚类综合质量评价指标\cite{ZhaoHen2007}是一种将聚类的密集和分离度集合起来的一种评价标准：
%
%\begin{equation}\label{equ:chapter2:Ocq}
%  Ocq(\beta)=1=(\beta\cdot Cmp+(1-\beta)\cdot Sep)
%\end{equation}
%
%其中，$Cmp$代表密集度，$Sep$代表分离度，$\beta$ 为调整系数，调整密集度和分离度的所占的比例权重，通常令$\beta=0.5$，保持二者有相同的权重。假设$Dev(X)$是数据集$X$的簇内方差，且聚类划分为$\mathbf{C}=\{c_1,c_2,\dots,c_M\}$，则有：
%
%\begin{equation}\label{equ:chapter2:Ocq:cmp}
%  Cmp=\frac{1}{M}\sum_{i=1}^M\frac{Dev(c_i)}{Dev(X)}
%\end{equation}
%
%\begin{equation}\label{equ:chapter2:Ocq:sep}
%  Sep=\frac{1}{M(M-1)}\sum_{i=1}^M\sum_{j=1,j\neq i}^M\exp\big[-\frac{d^2(x_{c_i},x_{c_j})}{2\delta^2}]
%\end{equation}
%
%为了计算方便，通常选择高斯常量$\delta$为满足$2\delta^2=1$，$x_{c_i}，x_{c_j}$是对应簇的中心，因此，根据定义很容易推导出$Ocq$的值和聚类效果成正比关系。


\subsection{半监督聚类}
半监督学习是介于无监督和监督学习之间的一种学习方法，待聚类的数据集中如果包含已标记的数据，则利用半监督学习方法可提高学习的性能。半监督学习按照功能不同可分为半监督分类和半监督聚类\cite{He2004}两种。半监督分类不仅利用有标记数据进行分类器训练，同时会加入无标记数据辅助训练分类器\cite{Chapelle2006}。半监督聚类利用少量先验知识(有标签的数据或数据间的约束信息)来指导聚类的过程。

先验知识即领域知识或背景知识，是基于数据集本身的约束信息，在半监督聚类方法中的先验知识形式可分为类标签和成对约束。

\begin{enumerate}[1)]
  \item 类标签

  对带有部分类标的数据集$X$可分为两部分$X^l$和$X^u$，$X=X^l\cup X^u$，并且$X^l\cup X^u=\varnothing$，其中$X^l=\{x^l_1,x^l_2,\dots,x^l_{n_l}\}$表示$n_l$ 个含有类标的对象集合，$X^u=\{x^u_1,x^u_2,\dots,x^u_{n_u}\}$ 表示$n_u$个没有类标的对象集合，一般$n_l$远小于$n_u$。

  \item 成对约束关系

  成对约束是半监督学习中更为普遍的一种先验知识，不同于类标签，成对约束不关心单个样本的标签，而是定义数据对象间的关联关系，即属于同一类或不属于同一类，通常Must-Link 约束和Cannot-Link约束来表示正关联约束和负关联约束。

  Must-Link约束表示数据集中的两个数据对象之间的相似度很大，在聚类的结果中应该被聚集在同一个簇中，Cannot-Link 约束反之。

  对于数据对象$x_i$和$x_j$，如果其所属的类分别是$C_i$ 和$C_j$，则基于$C_i$ 和$C_j$：

  \begin{equation}\label{equ:chapter2:link}
    (x_i,x_j)\in\left\{
    \begin{array}{cc}
      \text{Must-Link},  & if\;i=j \\
      \text{Cannot-Link}, & if\;i\neq j
    \end{array}
    \right.
  \end{equation}

  一般地，成对约束具有对称性以及传递性的特征：
  \begin{description}
    \item[(a)] 对称性
    \begin{equation}\label{equ:chapter2:link:symmetry}
    \begin{split}
      (x_i,x_j)\in \text{Must-Link} \Rightarrow & (x_j,x_i)\in \text{Must-Link}  \\
       (x_i,x_j)\in \text{Cannot-Link}  \Rightarrow  &(x_j,x_i)\in \text{Cannot-Link}
    \end{split}
    \end{equation}
    \item[(b)] 传递性
        \begin{equation}\label{equ:chapter2:link:transmit}
    \begin{split}
      (x_i,x_j)\in \text{Must-Link} \wedge  (x_j,x_k)\in \text{Must-Link} \Rightarrow & (x_i,x_j)\in \text{Must-Link} \\
       (x_i,x_j)\in \text{Cannot-Link} \wedge  (x_j,x_k)\in \text{Cannot-Link} \Rightarrow & (x_i,x_j)\in \text{Cannot-Link} \\
    \end{split}
    \end{equation}
  \end{description}
\end{enumerate}

半监督聚类是半监督学习中的一个重要组成部分，半监督聚类则是在无监督聚类的基础上，通过有标签数据(或约束关系)指导聚类过程，以提高聚类效果和质量\cite{GaoYing2008}。
%同时，文献\cite{Zhong2006}中指出，当少量的标记数据不足以反映大量未标记数据所蕴含的完整簇类结构时，半监督分类方法无法取代半监督聚类算法完成学习。
目前，半监督聚类算法在很多实际领域中已获得广泛应用，例如：图像处理、生物信息工程、文本挖掘等\cite{GaoYing2008,Zhong2006,Wagstaff2001,Huang2006,Chang2006}。 通过对现有的半监督聚类算法进行比较和分析，可以根据使用先验信息方法的不同，将半监督聚类算法大致归为一下三类\cite{GuanRenChu2010}:

%半监督分类和半监督聚类是半监督学习中的两个重要组成部分，两者的差别在于：半监督分类是在监督分类的基础上，通过无标记数据指导分类过程，以提高分类的准确性；半监督聚类则是在无监督聚类的基础上，通过标记数据(或约束关系)指导聚类过程，以提高聚类质量\cite{GaoYing2008}。同时，文献\cite{Zhong2006}中指出，当少量的标记数据不足以反映大量未标记数据所蕴含的完整簇类结构时，半监督分类方法无法取代半监督聚类算法完成学习。目前，半监督聚类算法在很多实际领域中已获得广泛应用，如：图像处理、文本挖掘、生物信息等\cite{GaoYing2008,Zhong2006,Wagstaff2001,Huang2006,Chang2006}。 通过对现有的半监督聚类算法进行比较和分析，可以根据使用先验信息方法的不同，将半监督聚类算法分成以下三类\cite{GuanRenChu2010}:


\begin{enumerate}[1)]
  \item 基于限制的方法：利用有监督信息来指导聚类过程来找到一个较好的数据划分。这种方法主要是通过有监督信息对聚类算法的收敛过程进行限制和约束，从而驱使它得到更好的结果。典型的算法有Seeded k-means算法和Constrained k-Means算法\cite{Basu2002}。
  \item 基于相似性度量的方法：首先训练对象间相似性度量用以满足限制信息，然后通过基于该相似性度量的聚类算法进行聚类。典型的算法有：Klein等人在Must-Link 和Cannot-Link对点限制的基础上，融合了二值传递关系方法的半监督聚类算法\cite{Klein2002}。
      %该方法首先使用求最短路径的方法添加must-link限制，然后在改变了的测度空间中仅施加样本层面上的cannot-link限制，从而实现了公式~\ref{equ:chapter2:link:transmit}传递闭包关系最后，利用完全链接层次聚类算法将cannot-link限制进行空间传播，最终达到利用了所有成对限制信息所造成的空间影响\cite{Klein2002}。
  \item 潜藏信息共同指导的方法：探索利用存在于数据集合本身可获得的聚类先验信息，如有监督信息和无监督信息，并将这种信息与已知信息相结合。典型的算法是：王玲等人提出的基于密度敏感的半监督谱聚类\cite{WangLing2007}。

      %该算法尝试将数据集中的空间一致性先验信息与成对限制信息相接合并引入到谱聚类算法中。所谓空间一致性先验信息主要包括局部一致性和全局一致性两个特征：
     % \begin{itemize}
%        \item 局部一致性--是指在空间位置上相邻的数据点具有较高的相似性；
%        \item 全局一致性--是指位于同一流形上的数据点具有较高的相似性。
%      \end{itemize}
%      三种数据集上实验表明，该算法比仅使用成对限制的半监督聚类算法在聚类性能上提高了很多。这种已知信息和未标记样本潜藏信息相互补充的方法将逐渐得到更为广泛的应用。
\end{enumerate}
%\begin{enumerate}[1)]
%  \item 基于限制的方法：该方法利用先验信息来修改聚类算法本身，从而指导这类算法找到一个较好的数据划分。这种方法主要通过先验信息对聚类算法的收敛过程进行限制，从而促使它得到更好的结果。典型的算法为Basu等人提出的Seeded k-means和Constrained k-means\cite{Basu2002}。 这两种算法在经典k-means算法的基础上，引入了由少量标记样本形成的种子集合，并且种子集中包含所有k个类别的样本，每类至少一个已标记样本。运用种子集对k-means算法进行初始化，并采用EM算法进行优化，最终形成两种基于k-means的半监督算法。
%  \item 基于相似性度量的方法。该方法首先训练相似性度量用以满足类属或限制信息，然后使用基于该相似性度量的聚类算法进行聚类。典型的算法为：Klein等人在must-link和cannot-link对点限制的基础上，融合了二值传递关系方法的半监督聚类算法。该方法首先使用求最短路径的方法添加must-link限制，然后在改变了的测度空间中仅施加样本层面上的cannot-link限制，从而实现了公式~\ref{equ:chapter2:link:transmit}传递闭包关系最后，利用完全链接层次聚类算法将cannot-link限制进行空间传播，最终达到利用了所有成对限制信息所造成的空间影响\cite{Klein2002}。
%  \item 已知信息和未标记样本潜藏信息共同指导的方法。该方法是探索利用存在于数据集合本身可获得的聚类先验信息，并将这种信息与已知信息相结合。典型的算法是：王玲等人提出的密度敏感的半监督谱聚类\cite{WangLing2007}。该算法尝试将数据集中的空间一致性先验信息与成对限制信息相接合并引入到谱聚类算法中。所谓空间一致性先验信息主要包括局部一致性和全局一致性两个特征：
%      \begin{itemize}
%        \item 局部一致性--是指在空间位置上相邻的数据点具有较高的相似性；
%        \item 全局一致性--是指位于同一流形上的数据点具有较高的相似性。
%      \end{itemize}
%      三种数据集上实验表明，该算法比仅使用成对限制的半监督聚类算法在聚类性能上提高了很多。这种已知信息和未标记样本潜藏信息相互补充的方法将逐渐得到更为广泛的应用。
%\end{enumerate}

\subsection{符号属性数据聚类算法}
%符号属性数据聚类算法的研究
针对数值属性数据聚类算法的研究已取得了丰硕的成果，然而，随着符号属性数据的不断增多，针对符号属性数据聚类算法的研究得到了越来越多的关注，并取得了一定的研究成果。符号属性聚类算法的模型可分为基于类型转换的聚类模型、基于相异测度的聚类模型、基于概率统计的聚类模型以及其它类型的模型等。
\begin{enumerate}[1)]
  \item 基于类型转换的聚类模型：将符号属性转化成连续型数值属性，然后通过现有的数值属性数据聚类算法对其进行聚类。如Ralambondrainy提出的Conceptual k-Means 聚类算法\cite{Ralambondrainy1995}，它将每个符号属性值变换到一个二值属性，然后利用k-Means聚类算法对其聚类。
      %但该算法有如下缺点:①符号属性值之间隐含的相似信息在新的关系中没有表现出来;② 由于改变了最初的属性名，编码属性没有表达正确的语义;③ 编码模式需要为二元属性增加额外存储，特别是当符号属性的值域较大的时候，导致存储空间和处理时间的增加;④ 类中心通过0和1之间的平均值来表示，无法表示类的特征;⑤ 属性值域是无序的。因而这种聚类算法不可取。
  \item 基于概率统计的聚类模型：针对符号属性的取值有限的特点，用概率统计来对其进行建模，将类原型定义为概率分布的形式，且对象与类间的相似性也用概率来表示，相应的聚类算法根据对象与类间的隶属概率来实现不同类的划分。通常这类聚类算法要用到概率统计中的Bayesian定理和极大似然估计法。COBWEB\cite{Fisher1987}，ECOBWEB\cite{Reich1991}和COP-COBWEB算法\cite{Wagstaff2000} 等。
  \item 基于相异测度的聚类模型：参照数值属性数据聚类算法的设计，重新定义适合于符号属性数据的相异测度，用它来代替距离测度，设计出类似于数值属性数据聚类算法。其中最具代表性的算法是由Huang在1997年提出的k-Modes聚类算法\cite{Huang1997}。它采用简单匹配差异法来计算符号属性数据之间的差异程度，并用Modes 代替k-Means算法中的均值，该算法将在以后的章节中详细介绍。另一种典型的算法是ROCK聚类算法\cite{Guha2000}，它根据相似度闭值和共享近邻来构建一个凝聚的层次聚类算法，得到样本集的聚类后在对整个数据集进行聚类。
      %它对K-Means聚类算法的扩展，采用简单匹配差异法来计算符号属性数据之间的差异程度，用Modes代替K-Means算法中的均值，通过基于频率的方法在聚类过程中不断更新Modes 使得聚类目标函数达到最小。随后又引入模糊概念产生了模糊K-Mode聚类算法\cite{Huang1999}。
      %另一种典型的算法是ROCK聚类算法\cite{Guha2000}，RocK聚类算法与K-Modes 聚类算法有很大的不同，它提出了“链”的概念，将类间的相似度定义为两类内所有元组对之间的“链”数，对数据进行抽样，根据相似度闭值和共享近邻的概念来构建一个凝聚的层次聚类算法，得到样本集的聚类后在对整个数据集进行聚类。

\item 除了以上的聚类模型外，有些学者还将嫡的概念引入进了符号属性数据聚类算法中。如Daniel从嫡的角度出发，认为类包含相似对象越多嫡越小，并将这一观点应用于符号属性数据聚类，提出了COOLCAT算法\cite{Barbara2002}。
    %chen等人\cite{Chen2005}利用增益嫡值来确定聚类的最佳数。

\end{enumerate}


\section{属性选择算法}
%数据挖掘中属性选择算法的分析与研究
属性选择问题是模式分类、数据挖掘、图像处理等许多不同领域的重点问题\cite{Blum1997}。近几年来，机器学习方法在实际应用中不断增长的重要性，使得属性选择问题成为十分热门的研究课题，广泛应用于模式识别、统计学、机器学习等领域，已经有很多国内外研究人员提出了独特的思想和解决方案\cite{Bhatt2005,Kohavi1997,Liu1996,Wang2007,Yu2004}。


%属性选择问题是模式分类、数据挖掘、图像处理等许多不同领域的重点问题\cite{Blum1997}。近几年来，机器学习方法在实际应用中不断增长的重要性，使得属性选择问题成为十分热门的研究课题，尤其是在考虑现实世界的数据挖掘，由于它不仅包含对象数量巨大，而且对象的维度也很高，属性选择就更加重要、。属性选择问题作为一个研究热点广泛应用于模式识别、统计学、机器学习等领域，已经有很多国内外研究人员提出了独特的思想和解决方案\cite{Bhatt2005,Kohavi1997,Liu1996,Wang2007,Yu2004}，但是从属性选择的实现方面来讲，属性选择是由属性评价和属性子集的搜索策略两部分构成的。

%有些数据属性对发现任务是没有影响的，这些属性选择和遗传算法属性的加入会大大影响挖掘效率，甚至还可能导致挖掘结果的偏差。因此，有效的缩减数据是很有必要的。数据简化是在对发现任务和数据本身内容理解的基础上寻找依赖于发现目标的表达数据的有用特征，以缩减数据规模，从而在尽可能保持数据原貌的前提下最大限度的精简数据量。它主要有两个途径:属性选择和数据抽样，分别针对数据库中的属性和记录。

属性选择是指在初始的$M$个属性中选择出一个有$m$($m$<$M$)个属性的属性子集，这$m$个属性可以像原来的$M$个属性一样用来正确区分数据集中的每个数据对象。随着域维度的增大，属性的数量也在不断增大。发现最优属性子集通常是难以实现的\cite{Kohavi1997}，许多与属性选择相关的问题都已被证明是NP-Hard问题\cite{Blum1992}。

%由于无关属性在多数机器学习方案中存在负面影响，通常在学习之前先进行属性选择，只保留一些最为相关的属性，而将其他属性都去除。选择相关属性最好的方法是人工选择，它是基于对学习问题的深入理解以及属性的真正含义而做出的选择。然而，自动的方法也是很有用的。通过去除不适当的属性以降低数据的维度，能改善学习算法的性能。还能提高速度，即使属性选择可能会带来很多计算。更重要的是，维度的降低能形成一个更为紧凑、更易理解的目标概念表达方式，使用户的注意力集中在最为相关的变量上。


%
%属性选择问题是模式分类、数据挖掘、图像处理等许多不同领域的重点问题\cite{Blum1997}。近几年来，机器学习方法在实际应用中不断增长的重要性，使得属性选择问题成为十分热门的话题，尤其是在考虑从现实世界的数据库或数据仓库中挖掘的时候，由于它不仅包含数量巨大的记录，而且包含大量与挖掘任务不相关的属性，属性选择就更加重要。有些数据属性对发现任务是没有影响的，这些属性选择和遗传算法属性的加入会大大影响挖掘效率，甚至还可能导致挖掘结果的偏差。因此，有效的缩减数据是很有必要的。数据简化是在对发现任务和数据本身内容理解的基础上寻找依赖于发现目标的表达数据的有用特征，以缩减数据规模，从而在尽可能保持数据原貌的前提下最大限度的精简数据量。它主要有两个途径:属性选择和数据抽样，分别针对数据库中的属性和记录。
%
%属性选择是指在初始的$N$个属性中选择出一个有$m(m<N)$ 个属性的子集，这$m$个属性可以像原来的$N$个属性一样用来正确区分数据集中的每个数据对象。随着域维度的增大，属性的数量也在不断增大。发现最优属性子集通常是难以实现的\cite{Kohavi1997}，许多与属性选择相关的问题都已被证明是NP-Hard问题\cite{Blum1992}。
%
%由于无关属性在多数机器学习方案中存在负面影响，通常在学习之前先进行属性选择，只保留一些最为相关的属性，而将其他属性都去除。选择相关属性最好的方法是人工选择，它是基于对学习问题的深入理解以及属性的真正含义而做出的选择。然而，自动的方法也是很有用的。通过去除不适当的属性以降低数据的维度，能改善学习算法的性能。还能提高速度，即使属性选择可能会带来很多计算。更重要的是，维度的降低能形成一个更为紧凑、更易理解的目标概念表达方式，使用户的注意力集中在最为相关的变量上。


%数据挖掘中约简技术与属性选择算法的研究

现有的属性选择算法大致可以分为三类：过滤(Filter)模型、封装(Wrapper)模型和混合(Hybrid) 模型。
\begin{enumerate}[1)]
  \item 过滤模型：定义属性间的度量来滤除无关属性和冗余属性，属性选择过程是独立于学习算法。这种模型的优点是耗时少，灵活性好。
  %一旦针对训练样例选出最佳属性子集，就可据此对大量的测试样例进行属性选择，从而降低实际应用问题中属性选择的代价。
  \item 封装模型：使用分类正确率作为评价函数\cite{Kohavi1997}。这种方法能够得到较高的分类预测精度，但由于在属性选择过程中必须使用学习算法对每一个搜索到的属性子集进行学习，需要耗费大量的时间，属性选择的效果依赖于分类器的好坏。
      %且改变学习算法时，又要重新进行属性选择，通用性不好。
  \item 混合模型：结合过滤模型和封装模型，首先采用过滤模型选择出多个候选属性子集，之后用封装模型在候选属性子集中选择最优属性子集。这种方法的时间复杂度、分类预测精度和适用性都介于过滤模型与封装模型之间。
\end{enumerate}

%
%现有的属性选择算法大致可以分为三类：过滤(Filter)模型、封装(Wrapper)模型和混合(Hybrid) 模型。
%\begin{enumerate}[1)]
%  \item 过滤模型：定义属性间的度量来滤除无关属性和冗余属性，属性选择过程是独立于学习算法。这种方法虽然损失了部分识别精度，但耗时少，灵活性好。一旦针对训练样例选出最佳属性子集，就可据此对大量的测试样例进行属性选择，从而降低实际应用问题中属性选择的代价。
%  \item 封装模型一般使用分类正确率作为评价函数。这种方法能够得到较高的识别精度，但由于在属性选择过程中必须使用学习算法对每一个搜索到的属性子集进行学习，需要耗费大量的时间，且改变学习算法时，又要重新进行属性选择，通用性不好。
%  \item 混合模型结合了上述两种模型的优点，首先采用过滤模型选择出多个候选属性子集，之后用封装模型在候选属性子集中选择最优属性子集。这种方法的时间复杂度、识别精度和适用性都介于过滤模型与封装模型之间。
%\end{enumerate}



%数据挖掘中属性选择算法的分析与研究

从属性选择的实现方面来讲，属性选择是由属性评价方法(Evaluation)和属性子集的搜索策略(Search Strategy)两部分构成的。
\subsection{属性评价方法}
属性评价方法主要包括：距离度量、信息度量、依赖性度量等。

\begin{enumerate}[1)]
  \item 距离度量:又称为可分离性、分歧法、区别度量法。对于两个类别的问题而言，选择属性$X$而不是选择属性$Y$，当使用$X$产生的两个类别的距离要比使用$Y$产生的距离大。如果差别是0，那么成$X$和$Y$是不可区分的。
      %欧几里德距离法就是属性选择和遗传算法一个例子。

  \item 依赖性度量:依赖性度量或相关性度量限定了依据一个变量的值来预测另一个值的能力。例如粗糙集里面的依赖度。
  %系数是一个能够用来发现属性值和类值之间关系的经典的依赖性度量。如果属性X和类值C之间的关系要比属性Y和类值C之间的关系强的话，就选择属性Xa这个方法稍微变化一下就可以用来计算一个属性和其他属性之间的依赖性，这个值可以用来衡量属性的冗余度。
  \item 信息度量:主要是值通过信息论的方法计算一个属性的信息增益。
  %属性X的信息增益被定义为使用X得到的先验概率和期望后验概率的差。当使用属性X 产生的信息增益比使用Y 产生的信息增益大时选择属性X而不选择属性Y(信息嫡度量)。
  %\item 一致性度量:这类方法和其他方法是不同的，因为它们严重依赖于训练数据集，在选择属性子集时还依赖于使用的最小特征偏见(Min-Feature bias)\cite{Almuallim1994}。Min-Feature偏见倾向于选择一致性假定，而不是选择尽可能少的属性。这些方法找到满足用户所设定的可接受矛盾率的最低限度的属性子集。
  %\item 分类错误率度量法:使用这一类评估函数的方法被称为“打包法”，例如分类器用作评估函数。由于使用分类器选择属性，然后这些选择出来的属性又被用来预测未知实例的类值，预测精度是很高的，但是计算花费非常大。
\end{enumerate}

%在对属性选择方法进行分类时，通常采用的方法是根据在搜索过程中是否使用到特定的学习算法而将属性选择分为过滤法和打包法。过滤法是一种独立于学习算法的属性选择方法，按照某种标准(如最小属性子集或者某种评分标准)进行属性的筛选，过滤法对学习算法的支持性不是很好，却以其计算的高效性，在高维属性选择中得到很好的应用\cite{Yu2004}。打包法则是将学习算法作为测试用的黑盒子，利用相关的学习算法对属性子集进行评价，例如，利用学习算法的交叉验证进行属性子集评价\cite{Kohavi1997}。此种方法需要一个属性子集搜索方法的支持。

%属性选择的评价方法主要包括：距离法、信息(不确定性)法、依赖性方法、一致性方法和分类错误率方法。
%
%\begin{enumerate}[1)]
%  \item 距离度量:又称为可分离性、分歧法、区别度量法。对于两个类别的问题而言，选择属性X而不是选择属性Y，当使用X产生的两个类别的距离要比使用Y产生的距离大。如果差别是0，那么成X和Y 是不可区分的。欧几里德距离法就是属性选择和遗传算法一个例子。
%  \item 信息度量:这些方法主要是计算一个属性的信息增益。属性X的信息增益被定义为使用X得到的先验概率和期望后验概率的差。当使用属性X 产生的信息增益比使用Y 产生的信息增益大时选择属性X而不选择属性Y(信息嫡度量)。
%  \item 依赖性度量:依赖性度量或相关性度量限定了依据一个变量的值来预测另一个值的能力。系数是一个能够用来发现属性值和类值之间关系的经典的依赖性度量。如果属性X和类值C之间的关系要比属性Y和类值C之间的关系强的话，就选择属性Xa 这个方法稍微变化一下就可以用来计算一个属性和其他属性之间的依赖性，这个值可以用来衡量属性的冗余度。
%  \item 一致性度量:这类方法和其他方法是不同的，因为它们严重依赖于训练数据集，在选择属性子集时还依赖于使用的最小特征偏见(Min-Feature bias)\cite{Almuallim1994}。Min-Feature偏见倾向于选择一致性假定，而不是选择尽可能少的属性。这些方法找到满足用户所设定的可接受矛盾率的最低限度的属性子集。
%  \item 分类错误率度量法:使用这一类评估函数的方法被称为“打包法”，例如分类器用作评估函数。由于使用分类器选择属性，然后这些选择出来的属性又被用来预测未知实例的类值，预测精度是很高的，但是计算花费非常大。
%\end{enumerate}
%
%在对属性选择方法进行分类时，通常采用的方法是根据在搜索过程中是否使用到特定的学习算法而将属性选择分为过滤法和打包法。过滤法是一种独立于学习算法的属性选择方法，按照某种标准(如最小属性子集或者某种评分标准)进行属性的筛选，过滤法对学习算法的支持性不是很好，却以其计算的高效性，在高维属性选择中得到很好的应用\cite{Yu2004}。打包法则是将学习算法作为测试用的黑盒子，利用相关的学习算法对属性子集进行评价，例如，利用学习算法的交叉验证进行
%属性子集评价\cite{Kohavi1997}。此种方法需要一个属性子集搜索方法的支持。


\subsection{属性子集的搜索策略}

属性子集的搜索策略主要可以分为：启发式搜索，穷尽式搜索和随机式搜索等。
\begin{enumerate}[1)]
  \item 启发式搜索：贪心的在每次循环中，所有剩余的属性都会被考虑用来选择(或丢弃)。这个简单的过程有很多种变化的方法，但产生的属性子集基本上是增加的(增加属性或减少属性)。搜索空间是$O(2^N)$ 或更少。
  \item 穷尽式搜索：通过计算评估函数对属性子集空间进行穷尽式搜索，搜索最优的属性子集。穷尽式的搜索就是完全的搜索。
  %然而，Schlimmer\cite{Schlimmer1993} 认为“搜索是完全的并不一定是穷尽式的搜索”。
  为了尽可能在不减少找到最优子集的概率的情况下，使用不同的启发函数被用来减少搜索所需要的时间。这样，尽管搜索属性空间仍旧是$O(2^N)$，但是评估的子集个数减少了。依赖于评估函数的最优属性子集能够被保证是因为在这个过程使用了回溯策略。回溯策略有很多种实现的方式，如:分店模式(Branch and Bound)，最优搜索(Best First Search)，宽度搜索(Beam Search)。

  \item 随机式搜索：通过设置一个可能大的搜索次数。搜索到的属性子集的最优程度依赖于可用的资源。在属性子空间中随机的选择，并通过一定的评价函数判断是否停止。如基于遗传算法的属性选择等。
\end{enumerate}

%属性选择的搜索方法主要可以分为以下三种:
%\begin{enumerate}
%  \item 穷尽式搜索:这种产生过程按照使用的评估函数对属性子集空间进行穷尽式搜索，以找到一个最优的属性子集。穷尽式的搜索就是完全的搜索。然而，Schlimmer\cite{Schlimmer1993}认为“搜索是完全的并不一定是穷尽式的搜索”。为了尽可能在不减少找到最优子集的概率的情况下，使用不同的启发函数被用来减少搜索的时间。因此，尽管搜索空间仍旧是$O(2^N)$，但是评估的子集个数减少了。依赖于评估函数的最优属性子集能够被保证是因为在这个过程使用了回退法。回退法有很多种实现的方式，如:分店模式(branch and bound)，最优搜索(best first search)，宽度搜索(beam search)。
%  \item 启发式搜索:在这种产生过程的每次循环中，所有剩余的属性都会被考虑用来选择(或丢弃)。这个简单的过程有很多种变化的方法，但产生的子集基本上是增加的(增加属性或减少属性)。搜索空间是$O(2^N)$或更少。比较例外的有Relief\cite{Kira1992}，DTM\cite{Cardie1993}。
%  \item 不确定搜索(随机式搜索):这种产生过程的方法和前两种方法比较起来都比较新。尽管搜索的空间还是$O(2^N)$，这一类中的方法搜索的属性子集个数都要少于$2^N$，通过设置一个可能大的搜索次数。搜索到的属性子集的最优程度依赖于可用的资源。每个随机的产生过程都需要设定一些参数。这些参数值设置的是否合适对能否达到较好的结果有很重要的作用。
%\end{enumerate}

\subsection{半监督属性选择算法}
%特征选择及半监督分类方法研究


%半监督特征选择方面的研究目前还比较少，主要通过两种方式给定先验知识，给定一个数据集$X=\{x_1,x_2,\ldots,x_i\}$，一种是利用少量标记样本$X^L\subset X$加上大量未标记样本$X^U\subset X$; 另一种是给定两个成对约束(Pairwise Constraints)集合，分别为必须相连(Must-link)集合$M$和不能相连(cannot-link)集合$C$，;然后加上大量未标记样本信息一起对特征或特征子集进行评价和选取。
半监督属性选择方面的研究目前还比较少，主要从两个方面考虑：
\begin{enumerate}[1)]
  \item 有标签数据和无标签数据结合

  Wu和Li\cite{Wu2003}利用直推式支持向量机\cite{Joachims1999}(TSVM，Transductive Support Vector Machine)方法进行属性选择，TSVM 利用了无标签数据的信息，通过逐步剔除权重小的特征达到属性选择的目的
  %但特征选择的基本思想来源于Guyon等人\cite{Guyon2002}: 通过逐步剔除权重小的特征达到特征选择的目的；
  Zhao和Liu\cite{Zhao2007} 提出了一种基于谱分析的半监督特征选择方法，通过评价聚类可分性和一致性对所有属性进行评分。
  %该方法利用一个正则化框架来利用标记样本和未标一记样本，主要引入了一个聚类指示器 (Cluster Indictors) 的概念，然后通过评价聚类可分性和一致性对所选特征进行评分。在Sun等人提出的逻辑迭代Relief方法(Logistic I-RELIEF)\cite{Sun2008}的基础上，
  Yubo等人 \cite{Cheng2008} 在目标函数中添加未标记样本信息以实现半监督属性选择。
  %Ren\cite{Ren2008}等人提出了一种包装法类型的半监督特征选择方法，该方法的基本思想是:选择几个特征作为初始的特征集，然后利用这个特征集在标记样本上训练，训练完后在未标记样本集上进行预测，随机的从未标记样本集中选一定比例的数据加入到标记样本集中，形成新的训练集。不断的随机评价所选特征，特征选择就在随机评价过程中进行，选择出现频率最高的一个特征加入到当前的特征集中，算法不断循环直至结束，最终获得特征子集。
  Yaslan等人\cite{Yaslan2010} 利用选取两个相关性属性子集对Co-Training算法\cite{Blum1998}进行改进。
  %Yaslan等人\cite{Yaslan2010} 利用选取两个相关性属性子集对Co-Training算法\cite{Blum1998}(之前的两属性子集往往是随机划分的)进行改进。

  \item 成对约束与无标签数据结合
  Zhang等人\cite{Zhang2008}提出了一种用于属性选择的新半监督评价方法，CS方法，该方法通过利用成对约束集来引导属性选择，并给出了一个约束分数(CS,Constraint Score)。
  %该文通过实验对著名的 Fisher Score和 Laplacian Score方法进行了比较分析。结果表明CS方法只需要少量成对约束就可以获得比较好的结果。该文还利用图谱理论进行了扩展。但该方法只是对每个特征进行了分析，没有对特征子集进行分析。另一个缺点就是，特征的评价不太稳定，与成对约束集的选取有很大关系。
  %针对这一缺点，
  Sun和Zhang提出了BCS(Bagging Constrained Score)方法\cite{Sun2010}，将Bagging方法引入其中，利用多个约束集来代替原有单一约束集的想法，并通过投票表决的方式学习属性选择空间。
  %，成为一种利用原有CS评分准则的一种集成方法，但这两篇论文都没有结合未标一记样本信息，仅仅在成对约束集M集和C 集上进行特征评价和排序，这样容易造成评价结果的不稳定。


\end{enumerate}



%半监督特征选择方面的研究目前还比较少，主要通过两种方式给定先验知识，给定一个数据集$X=\{x_1,x_2,\ldots,x_i\}$，一种是利用少量标记样本$X^L\subset X$加上大量未标记样本$X^U\subset X$; 另一种是给定两个成对约束(Pairwise Constraints)集合，分别为必
%须相连(Must-link)集合$M$和不能相连(cannot-link)集合$C$，;然后加上大量未标记样本信息一起对特征或特征子集进行评价和选取。
%\begin{itemize}
%  \item 标记样本与未标记样本结合
%
%  Wu和Li\cite{Wu2003}利用直推式支持向量机\cite{Joachims1999}(TSVM，Transductive Support Vector Machine)方法进行特征选择，TSVM 利用了未标记样本的信息，但特征选择的基本思想来源于Guyon等人\cite{Guyon2002}: 通过逐步剔除权重小的特征达到特征选择的目的；Zhao和Liu\cite{Zhao2007} 提出了一种基于谱分析的半监督特征选择方法。该方法利用一个正则化框架来利用标记样本和未标一记样本，主要引入了一个聚类指示器 (Cluster Indictors) 的概念，然后通过评价聚类可分性和一致性对所选特征进行评分。在Sun 等人提出的逻辑迭代Relief方法(Logistic I-RELIEF)\cite{Sun2008}的基础上，Yubo等人 \cite{Cheng2008} 在目标函数中添加未标记样本信息以实现半监督特征选择。Ren\cite{Ren2008}等人提出了一种包装法类型的半监督特征选择方法，该方法的基本思想是:选择几个特征作为初始的特征集，然后利用这个特征集在标记样本上训练，训练完后在未标记样本集上进行预测，随机的从未标记样本集中选一定比例的数据加入到标记样本集中，形成新的训练集。不断的随机评价所选特征，特征选择就在随机评价过程中进行，选择出现频率最高的一个特征加入到当前的特征集中，算法不断循环直至结束，最终获得特征子集。Yaslan等人\cite{Yaslan2010}利用选取两个相关性特征子集对Co-Training算法\cite{Blum1998}(之前的两特征子集往往是随机划分的)进行改进。
%
%  \item 成对约束集与未标记样本结合
%  Zhang等人\cite{Zhang2008}提出了一种用于特征选择的新半监督评价方法，CS方法，该方法通过利用成对约束集来引导特征选择，并给出了一个约束分数(CS，Constraint Score)。该文通过实验对著名的 Fisher Score和 Laplacian Score方法进行了比较分析。结果表明CS方法只需要少量成对约束就可以获得比较好的结果。该文还利用图谱理论进行了扩展。但该方法只是对每个特征进行了分析，没有对特征子集进行分析。另一个缺点就是，特征的评价不太稳定，与成对约束集的选取有很大关系。
%
%  针对这一缺点，Sun和Zhang又提出了 BCS(Bagging Constrained Score)方法BCS\cite{Sun2010}，将Bagging方法引入其中，利用多个约束集来代替原有单一约束集的想法，并通过投票表决的方式学习假设空间，成为一种利用原有CS评分准则的一种集成方法，但这两篇论文都没有结合未标一记样本信息，仅仅在成对约束集M集和C 集上进行特征评价和排序，这样容易造成评价结果的不稳定。
%
%
%\end{itemize}



\section{小结}
本章主要介绍了聚类和属性选择的基本概念，
介绍了聚类算法划分方法和半监督聚类的研究现状，然后又讲了符号属性数据的相关聚类算法。接着对属性选择进行了介绍，并对属性选择两方面问题：属性评价和搜索策略进行了阐述。最后对半监督属性选择算法的研究成果做了总结。可以看出，半监督聚类和属性选择在符号属性数据的方法相对较少，有些算法并不适合符号属性数据，对此，本文提出了几种解决方案，并进行了实验验证。
